{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25bb19a2",
   "metadata": {},
   "source": [
    "\n",
    "# Use PiCaS to run local tasks\n",
    "\n",
    "## Outline\n",
    "- Ensure the environment is set up correctly.\n",
    "- Set up a simple problem\n",
    "- Create tokens and push them to the database\n",
    "- Run the tasks locally using PiCaS by pulling tokens from the database\n",
    "\n",
    "## references\n",
    "- the main couchdb database: https://picas.surfsara.nl:6984\n",
    "- the couchdb web ui login page: https://picas.grid.sara.nl:6984/_utils/#login\n",
    "- the couchdb database for your project: https://picas.grid.sara.nl:6984/_utils/#database/<my_project_db>/_all_docs\n",
    "- the push token example https://github.com/sara-nl/picasclient/blob/master/examples/pushTokens.py\n",
    "- the run local examples script https://github.com/sara-nl/picasclient/blob/master/examples/local_example.py\n",
    "\n",
    "## Minimum requirements\n",
    "- a configured environemt with login credentials and the picas configuration file\n",
    "- The previous to notebooks: 00-environment-setup and 01-database-setup should have been run.\n",
    "- cd ~/workspaces/surf/picas/picasclient\n",
    "- source .venv/picas-tutorial/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/picas\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir example_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957262a",
   "metadata": {},
   "source": [
    "# Initialize the picas configuration (if not already done)\n",
    "See the 01-database-setup ( ..todo:: add hyperlink) notebook for details on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d587b",
   "metadata": {},
   "source": [
    "## Hello world PiCaS example\n",
    "The simplest problem that can be run locally is to pass parameters (tokens) to a script and run it.\n",
    "The example below depicts a typical workflow on a cluster, i.e a script that accepts one or more\n",
    "parameters and runs a command with those parameters within the pilot job (that is already allocated\n",
    "and running on the cluster). In the example below the script is run locally. In the next tutorials\n",
    "the jobs will be run on a cluster using slurm.\n",
    "\n",
    "The steps and components are:\n",
    "  - The script that runs a tasks (e.g. `process_task.sh`): just accept the input parameters and print them.\n",
    "  - The set of parameters (tokens) that first need to be pushed to the database, e.g quickExamples.txt\n",
    "  - The script that pushes the tokens to the database ( e.g. `pushTokens.py`).\n",
    "  - The script that is executed locally that pulls the tokens from the database and runs the task (e.g. `local_example.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f4345",
   "metadata": {},
   "source": [
    "### The script that runs a task: process_task.sh\n",
    "The script basically accepts the tokens as input command line parameters.\n",
    "The following is done in the script:\n",
    " - display the node name and date\n",
    " - initialize the job arguments and echo them (for verbosity)\n",
    " - for the sake of demonstration, run the input command as a bash command\n",
    " - wrap up the job by displaying the end date and exit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile example_02/process_task.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# usage\n",
    "#  ./process_task.sh <input_command> <token_id>\n",
    "#  ./process_task.sh 'sleep 1' my_token_id\n",
    "# enable verbosity\n",
    "#set -x\n",
    "\n",
    "# obtain/dump the information for the Worker Node to stdout\n",
    "echo \"\"\n",
    "echo `date`\n",
    "echo ${HOSTNAME}\n",
    "\n",
    "# initialize job arguments\n",
    "INPUT_CMD=$1\n",
    "TOKENID=$2\n",
    "OUTPUT=output_${TOKENID}\n",
    "echo \"----------- input argument ----------------\"\n",
    "echo \"Input command: ${INPUT_CMD}\"\n",
    "echo \"Token ID: ${TOKENID}\"\n",
    "echo \"Output file: ${OUTPUT}\"\n",
    "echo \"------------ end input argument ---------------\"\n",
    "\n",
    "#\n",
    "# start processing\n",
    "#\n",
    "\n",
    "# short example, just echo the input\n",
    "# use this command for the short example, replace this with something else\n",
    "# that does fancy things for a real life application\n",
    "# .. just run something, dummy task\n",
    "echo \"----------------- start execute task --------------------------------\"\n",
    "bash -c \"${INPUT_CMD}\" || { echo `date`; echo \"Task failed\"; exit 1; }\n",
    "echo \"----------------- end execute task --------------------------------\"\n",
    "\n",
    "# display the end date\n",
    "echo `date`\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e806b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the execute permission on the script\n",
    "! chmod +x example_02/process_task.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b03f9",
   "metadata": {},
   "source": [
    "### Define the bunch of parameters for the tasks.\n",
    "In the file below, each line is one parameter of the task. This file will be processed by the\n",
    "push tokens script and the content of the file will be translated into tokens to be consumed\n",
    "later on by the main run script that pulls the tokens and dispatches them as tasks in the pilot\n",
    "allocated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb7028",
   "metadata": {},
   "source": [
    "This file defined the parameters for the tasks (the tokens).\n",
    "in this case the tokens are simple echo commands that will be passed by PiCaS\n",
    "to the runner of the tasks (see the \"local_example\" below, the process_task method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fa99c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%writefile example_02/quickExample.txt\n",
    "echo 'this is token A'\n",
    "echo 'this is token B'\n",
    "echo 'this is token C'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca66b3",
   "metadata": {},
   "source": [
    "The push tokens script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile example_02/push_tokens.py\n",
    "\n",
    "import picasconfig\n",
    "from picas.clients import CouchDB\n",
    "from picas.documents import Task\n",
    "\n",
    "def create_tokens(fields: dict) -> list:\n",
    "    \"\"\"\n",
    "    Create the tokens as a list of Task documents.\n",
    "\n",
    "    The fields parameter is a dictionary where keys are field names and values are\n",
    "    lists of input values. For every 'input' value a unique id is assigned and the\n",
    "    corresponding input value is used to create a token.\n",
    "\n",
    "    For example, the following becomes a list of tokens:\n",
    "     {'input': [\n",
    "        \"echo 'this is token A'\",\n",
    "        \"echo 'this is token B'\",\n",
    "        \"echo 'this is token C'\"]\n",
    "     }\n",
    "\n",
    "\n",
    "    :param fields: A dictionary where keys are field names and values are lists of input values.\n",
    "    :return: A list of Task documents representing the tokens. For the example above,\n",
    "      it would return a list of three Task objects with .id attributes set to\n",
    "      'token_0', 'token_1', and 'token_2' respectively and .input attributes of each set to\n",
    "      \"echo 'this is token A'\", \"echo 'this is token B'\", and \"echo 'this is token C'\".\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    n_docs = db.doc_count()\n",
    "    for arg in fields:\n",
    "        for line in fields[arg]:\n",
    "            token = {\n",
    "                '_id': 'token_' + str(n_docs),\n",
    "                'type': 'token',\n",
    "                arg: line,\n",
    "            }\n",
    "            tokens.append(Task(token))\n",
    "            n_docs += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# create a connection to the server\n",
    "db = CouchDB(\n",
    "    url=picasconfig.PICAS_HOST_URL,\n",
    "    db=picasconfig.PICAS_DATABASE,\n",
    "    username=picasconfig.PICAS_USERNAME,\n",
    "    password=picasconfig.PICAS_PASSWORD)\n",
    "\n",
    "# create tokens with inputs given in file\n",
    "tokensfile = \"quickExample.txt\"\n",
    "with open(tokensfile) as fobj:\n",
    "    fields = {\"input\": fobj.read().splitlines()}\n",
    "tokens = create_tokens(fields)\n",
    "\n",
    "# save tokens in database\n",
    "db.save_documents(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd example_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03136d27",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13454b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import picasconfig\n",
    "from picas.clients import CouchDB\n",
    "from picas.documents import Task\n",
    "\n",
    "def create_tokens(fields: dict) -> list:\n",
    "    \"\"\"\n",
    "    Create a list of Tasks from the tokens.\n",
    "\n",
    "    The fields parameter is a dictionary where keys are field names and values are\n",
    "    lists of input values. For every 'input' value a unique id is assigned and the\n",
    "    corresponding input value is used to create a token.\n",
    "\n",
    "    For example, the following becomes a list of tokens:\n",
    "     {'input': [\n",
    "        \"echo 'this is token A'\",\n",
    "        \"echo 'this is token B'\",\n",
    "        \"echo 'this is token C'\"]\n",
    "     }\n",
    "\n",
    "\n",
    "    :param fields: A dictionary where keys are field names and values are lists of input values.\n",
    "    :return: A list of Task documents representing the tokens. For the example above,\n",
    "      it would return a list of three Task objects with .id attributes set to\n",
    "      'token_0', 'token_1', and 'token_2' respectively and .input attributes of each set to\n",
    "      \"echo 'this is token A'\", \"echo 'this is token B'\", and \"echo 'this is token C'\".\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    n_docs = db.doc_count()\n",
    "    for arg in fields:\n",
    "        for line in fields[arg]:\n",
    "            token = {\n",
    "                '_id': 'token_' + str(n_docs),\n",
    "                'type': 'token',\n",
    "                arg: line,\n",
    "            }\n",
    "            tokens.append(Task(token))\n",
    "            n_docs += 1\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a56863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection to the server\n",
    "db = CouchDB(\n",
    "    url=picasconfig.PICAS_HOST_URL,\n",
    "    db=picasconfig.PICAS_DATABASE,\n",
    "    username=picasconfig.PICAS_USERNAME,\n",
    "    password=picasconfig.PICAS_PASSWORD)\n",
    "\n",
    "# create tokens with inputs given in file\n",
    "tokensfile = \"quickExample.txt\"\n",
    "with open(tokensfile) as fobj:\n",
    "    fields = {\"input\": fobj.read().splitlines()}\n",
    "tokens = create_tokens(fields)\n",
    "\n",
    "# save tokens in database\n",
    "status = db.save_documents(tokens)\n",
    "\n",
    "# check for errors\n",
    "if not any(status):\n",
    "    print(\"Tokens saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060aeb3",
   "metadata": {},
   "source": [
    "The picas local_example.py orchestrator script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile example_02/local_example.py\n",
    "\"\"\"\n",
    "usage: python local_example.py\n",
    "description:\n",
    "    Connect to PiCaS server\n",
    "    Get the next token in todo View\n",
    "    Fetch the token parameters, e.g. input value\n",
    "    Run main job (process_task.sh) with the input argument\n",
    "    When done, return the exit code to the token\n",
    "    Attach the logs to the token\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import picasconfig\n",
    "\n",
    "from picas.actors import RunActor\n",
    "from picas.clients import CouchDB\n",
    "from picas.executers import execute\n",
    "from picas.modifiers import BasicTokenModifier\n",
    "from picas.util import Timer\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def arg_parser():\n",
    "    \"\"\"\n",
    "    Arguments parser for optional values of the example\n",
    "    returns: argparse object\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Arguments used in the different classes in the example.\")\n",
    "    parser.add_argument(\"--design_doc\", default=\"Monitor\", type=str, help=\"Select the designdoc used by the actor class\")\n",
    "    parser.add_argument(\"--view\", default=\"todo\", type=str, help=\"Select the view used by the actor class\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Set verbose\")\n",
    "    return parser\n",
    "\n",
    "class ExampleActor(RunActor):\n",
    "    \"\"\"\n",
    "    The ExampleActor is the custom implementation of a RunActor that the user needs for the processing.\n",
    "    Feel free to adjust to whatever you need, a template can be found at: example-template.py\n",
    "    \"\"\"\n",
    "    def __init__(self, db, modifier, view=\"todo\", **viewargs):\n",
    "        super(ExampleActor, self).__init__(db, view=view, **viewargs)\n",
    "        self.timer = Timer()\n",
    "        # self.iterator = EndlessViewIterator(self.iterator)\n",
    "        self.modifier = modifier\n",
    "        self.client = db\n",
    "\n",
    "    def process_task(self, token):\n",
    "        # Print token information\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Working on token: \" +token['_id'])\n",
    "        for key, value in token.doc.items():\n",
    "            print(key, value)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        # Start running the main job, the logging is done internally and saved below\n",
    "        command = [\"/usr/bin/time\", \"./process_task.sh\", token['input'], token['_id']]\n",
    "        out = execute(command)\n",
    "\n",
    "        logsout = f\"logs_{token['_id']}.out\"\n",
    "        logserr = f\"logs_{token['_id']}.err\"\n",
    "\n",
    "        # write the logs\n",
    "        with open(logsout, 'w') as f:\n",
    "            f.write(out[2].decode('utf-8'))\n",
    "        with open(logserr, 'w') as f:\n",
    "            f.write(out[3].decode('utf-8'))\n",
    "\n",
    "        self.subprocess = out[0]\n",
    "\n",
    "        # Get the job exit code and done in the token\n",
    "        token['exit_code'] = out[1]\n",
    "        token = self.modifier.close(token)\n",
    "\n",
    "        # Attach logs in token\n",
    "        curdate = time.strftime(\"%d/%m/%Y_%H:%M:%S_\")\n",
    "        try:\n",
    "            log_handle = open(logsout, 'rb')\n",
    "            token.put_attachment(logsout, log_handle.read())\n",
    "\n",
    "            log_handle = open(logserr, 'rb')\n",
    "            token.put_attachment(logserr, log_handle.read())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # parse user arguments\n",
    "    args = arg_parser().parse_args()\n",
    "\n",
    "    # setup connection to db\n",
    "    client = CouchDB(\n",
    "        url=picasconfig.PICAS_HOST_URL,\n",
    "        db=picasconfig.PICAS_DATABASE,\n",
    "        username=picasconfig.PICAS_USERNAME,\n",
    "        password=picasconfig.PICAS_PASSWORD)\n",
    "    print(\"Connected to the database %s sucessfully. Now starting work...\" %(picasconfig.PICAS_DATABASE))\n",
    "\n",
    "    # create the token modifier\n",
    "    modifier = BasicTokenModifier()\n",
    "\n",
    "    # create the actor\n",
    "    actor = ExampleActor(client, modifier, view=args.view, design_doc=args.design_doc)\n",
    "\n",
    "    # start the work!\n",
    "    actor.run(max_token_time=1800, max_total_time=3600, max_tasks=10, max_scrub=2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a397d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import picasconfig\n",
    "\n",
    "from picas.actors import RunActor\n",
    "from picas.clients import CouchDB\n",
    "from picas.executers import execute\n",
    "from picas.modifiers import BasicTokenModifier\n",
    "from picas.util import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419fd797",
   "metadata": {},
   "source": [
    "The goal is to pass the parameters (tokens) to the .sh script and run it.\n",
    "PiCaS is responsible and will take care of fetching the tokens.\n",
    "The user's responsibility is implement the \"process_task\" method that PiCaS will call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169eedd",
   "metadata": {},
   "source": [
    "To customize the processing, the user needs to implement the process_task method\n",
    "https://github.com/sara-nl/picasclient/blob/master/examples/example_template.py\n",
    "see actual example in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574613a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExampleActor(RunActor):\n",
    "    \"\"\"\n",
    "    The ExampleActor is the custom implementation of a RunActor that the user needs for the processing.\n",
    "    Feel free to adjust to whatever you need, a template can be found at: example-template.py\n",
    "    \"\"\"\n",
    "    def __init__(self, db, modifier, view=\"todo\", **viewargs):\n",
    "        super(ExampleActor, self).__init__(db, view=view, **viewargs)\n",
    "        self.timer = Timer()\n",
    "        self.modifier = modifier\n",
    "        self.client = db\n",
    "\n",
    "    def process_task(self, token):\n",
    "        # Print token information\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Working on token: \" +token['_id'])\n",
    "        for key, value in token.doc.items():\n",
    "            print(key, value)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        # Start running the main job, the logging is done internally and saved below\n",
    "        command = [\"/usr/bin/time\", \"./process_task.sh\", token['input'], token['_id']]\n",
    "        out = execute(command)\n",
    "\n",
    "        logsout = f\"logs_{token['_id']}.out\"\n",
    "        logserr = f\"logs_{token['_id']}.err\"\n",
    "\n",
    "        # write the logs\n",
    "        with open(logsout, 'w') as f:\n",
    "            f.write(out[2].decode('utf-8'))\n",
    "        with open(logserr, 'w') as f:\n",
    "            f.write(out[3].decode('utf-8'))\n",
    "\n",
    "        self.subprocess = out[0]\n",
    "\n",
    "        # Get the job exit code and done in the token\n",
    "        token['exit_code'] = out[1]\n",
    "        token = self.modifier.close(token)\n",
    "\n",
    "        # Attach logs in token\n",
    "        curdate = time.strftime(\"%d/%m/%Y_%H:%M:%S_\")\n",
    "        try:\n",
    "            log_handle = open(logsout, 'rb')\n",
    "            token.put_attachment(logsout, log_handle.read())\n",
    "\n",
    "            log_handle = open(logserr, 'rb')\n",
    "            token.put_attachment(logserr, log_handle.read())\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup connection to db (same as before)\n",
    "client = CouchDB(\n",
    "    url=picasconfig.PICAS_HOST_URL,\n",
    "    db=picasconfig.PICAS_DATABASE,\n",
    "    username=picasconfig.PICAS_USERNAME,\n",
    "    password=picasconfig.PICAS_PASSWORD)\n",
    "print(\"Connected to the database %s sucessfully. Now starting work...\" %(picasconfig.PICAS_DATABASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the token modifier\n",
    "modifier = BasicTokenModifier()\n",
    "\n",
    "# create the actor\n",
    "actor = ExampleActor(client, modifier, view='todo', design_doc='Monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bf52f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# start the work!\n",
    "actor.run(max_token_time=1800, max_total_time=3600, max_tasks=10, max_scrub=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda1621",
   "metadata": {},
   "source": [
    " - semi-advanced example 1\n",
    "  - add an example of a top level dir that has a bunch of subdirs, each subdir has a some raw data\n",
    "  - the directory tree is traversed and each subdir is processed as a token\n",
    "  - the task would return a word count of the files in the subdir\n",
    " - advanced example 1\n",
    "  - same as above, but the raw data is pulled from dcache\n",
    "- todo:"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
