{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98068cd",
   "metadata": {},
   "source": [
    "\n",
    "# Use PiCaS to run tasks on Spider\n",
    "\n",
    "## Outline\n",
    "- Set up a non trivial example - count the number of words in text files\n",
    "- Create tokens and push them to the database\n",
    "- Run the tasks on Spider using PiCaS by pulling tokens from the database\n",
    "\n",
    "## references\n",
    "- 02-local-run.ipynb notebook\n",
    "\n",
    "## Minimum requirements\n",
    "- you have run the 02-local-run.ipynb notebook and have a working PiCaS database\n",
    "- you have access to Spider (works also on Snellius but you need to update the slurm script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff67592",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/picas_tutorial\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eada7e",
   "metadata": {},
   "source": [
    "## Generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install faker tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18daeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from faker import Faker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188057f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------------- CONFIG ----------------------\n",
    "ROOT_DIR = Path(\"/dev/shm/picas_fake_data\")  # create data in shared memory for faster access\n",
    "SEED = None  # set to an int for reproducibility, e.g., 42\n",
    "\n",
    "TOP_LEVEL_COUNTRIES = 5\n",
    "MAX_CITIES_PER_COUNTRY = 5\n",
    "MAX_FIRSTNAMES_PER_CITY = 5\n",
    "MAX_LASTNAMES_PER_FIRSTNAME = 5\n",
    "\n",
    "FILES_PER_LEAF = 10          # set up to 100 if desired\n",
    "MIN_WORDS_PER_FILE = 10\n",
    "MAX_WORDS_PER_FILE = 1000\n",
    "# ----------------------------------------------------\n",
    "\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "\n",
    "# Faker instances: English names for people, general for places\n",
    "fake_global = Faker()\n",
    "fake_en = Faker(\"en_US\")\n",
    "if SEED is not None:\n",
    "    fake_global.seed_instance(SEED)\n",
    "    fake_en.seed_instance(SEED + 1 if isinstance(SEED, int) else None)\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Make a filesystem-safe folder/file name.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[\\/\\\\:*?\\\"<>|]\", \"\", name)  # remove reserved characters\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    return name or \"unnamed\"\n",
    "\n",
    "def unique_items(generator, count):\n",
    "    \"\"\"Return up to 'count' unique items from a callable generator().\"\"\"\n",
    "    seen = set()\n",
    "    items = []\n",
    "    attempts = 0\n",
    "    while len(items) < count and attempts < count * 20:\n",
    "        attempts += 1\n",
    "        val = generator()\n",
    "        if val not in seen:\n",
    "            seen.add(val)\n",
    "            items.append(val)\n",
    "    return items\n",
    "\n",
    "def gen_words(n: int) -> str:\n",
    "    \"\"\"Generate n 'readable' lorem-like words with occasional punctuation.\"\"\"\n",
    "    words = fake_global.words(nb=n)\n",
    "    out = []\n",
    "    counter = 0\n",
    "    break_at = random.randint(12, 20)\n",
    "    for w in words:\n",
    "        out.append(w)\n",
    "        counter += 1\n",
    "        if counter >= break_at:\n",
    "            out[-1] = out[-1] + \".\"\n",
    "            counter = 0\n",
    "            break_at = random.randint(12, 20)\n",
    "    text = \" \".join(out)\n",
    "    text = \". \".join(s.strip().capitalize() for s in text.split(\".\") if s.strip())\n",
    "    if not text.endswith(\".\"):\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def generate_data():\n",
    "    ensure_dir(ROOT_DIR)\n",
    "\n",
    "    # Pre-calculate the directory structure\n",
    "    structure = []\n",
    "    total_files = 0\n",
    "\n",
    "    countries = unique_items(lambda: sanitize(fake_global.country()), TOP_LEVEL_COUNTRIES)\n",
    "\n",
    "    for country in countries:\n",
    "        country_dir = ROOT_DIR / country\n",
    "        num_cities = random.randint(1, MAX_CITIES_PER_COUNTRY)\n",
    "        cities = unique_items(lambda: sanitize(fake_global.city()), num_cities)\n",
    "\n",
    "        for city in cities:\n",
    "            city_dir = country_dir / city\n",
    "            num_first = random.randint(1, MAX_FIRSTNAMES_PER_CITY)\n",
    "            first_names = unique_items(lambda: sanitize(fake_en.first_name()), num_first)\n",
    "\n",
    "            for first_name in first_names:\n",
    "                first_dir = city_dir / first_name\n",
    "                num_last = random.randint(1, MAX_LASTNAMES_PER_FIRSTNAME)\n",
    "                last_names = unique_items(lambda: sanitize(fake_en.last_name()), num_last)\n",
    "\n",
    "                for last_name in last_names:\n",
    "                    leaf_dir = first_dir / last_name\n",
    "                    structure.append(leaf_dir)\n",
    "                    total_files += FILES_PER_LEAF\n",
    "\n",
    "    print(f\"Generating {total_files} files in {len(structure)} directories...\")\n",
    "\n",
    "    # Create all directories and files with a single progress bar\n",
    "    with tqdm(total=total_files, desc=\"Creating files\", unit=\"file\") as pbar:\n",
    "        for leaf_dir in structure:\n",
    "            ensure_dir(leaf_dir)\n",
    "\n",
    "            for i in range(1, FILES_PER_LEAF + 1):\n",
    "                fname = f\"file_{i:03d}.txt\"\n",
    "                fpath = leaf_dir / fname\n",
    "                word_count = random.randint(MIN_WORDS_PER_FILE, MAX_WORDS_PER_FILE)\n",
    "                content = gen_words(word_count)\n",
    "                fpath.write_text(content, encoding=\"utf-8\")\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"\\nâœ… Done. Created {total_files} files in structure under: {ROOT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e200285",
   "metadata": {},
   "source": [
    "## CONTINUE FROM HERE\n",
    "- create and push the tokens in python\n",
    "- modify the process_task.sh to execute the word count python script (create it)\n",
    "- create the process_text_file.py that does something to the text files (e.g. count words, or summarization)\n",
    "- add option to summarize the text file content ( separate heading - prioritize the word count)\n",
    "- add the script to submit to slurm on spider (spider_example.sh)\n",
    "- submit the job\n",
    "- wait for all to finish\n",
    "- fetch the results from the db and visualize them (word count histogram)\n",
    "- for the summaization maybe only do a sentiment analysis and show a histogram of that"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
