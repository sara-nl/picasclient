{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d4f000",
   "metadata": {},
   "source": [
    "\n",
    "# Use PiCaS to run tasks on Spider\n",
    "\n",
    "## Outline\n",
    "- Set up a non trivial example - count the number of words in text files\n",
    "- Create tokens and push them to the database\n",
    "- Run the tasks on Spider using PiCaS by pulling tokens from the database\n",
    "\n",
    "## references\n",
    "- 02-local-run.ipynb notebook\n",
    "\n",
    "## Minimum requirements\n",
    "- you have run the 02-local-run.ipynb notebook and have a working PiCaS database\n",
    "- you have access to Spider (works also on Snellius but you need to update the slurm script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c29a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/picas_tutorial/picasclient/examples\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5153490",
   "metadata": {},
   "source": [
    "## Generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install faker tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from faker import Faker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80616c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"~/picas_tutorial/picas_fake_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cef4fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(data_dir)\n",
    "# ---------------------- CONFIG ----------------------\n",
    "ROOT_DIR = Path(os.path.expanduser(data_dir))\n",
    "SEED = None  # set to an int for reproducibility, e.g., 42\n",
    "\n",
    "TOP_LEVEL_COUNTRIES = 5\n",
    "MAX_CITIES_PER_COUNTRY = 5\n",
    "MAX_FIRSTNAMES_PER_CITY = 5\n",
    "MAX_LASTNAMES_PER_FIRSTNAME = 5\n",
    "\n",
    "FILES_PER_LEAF = 10          # set up to 100 if desired\n",
    "MIN_WORDS_PER_FILE = 10\n",
    "MAX_WORDS_PER_FILE = 1000\n",
    "# ----------------------------------------------------\n",
    "\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "\n",
    "# Faker instances: English names for people, general for places\n",
    "fake_global = Faker()\n",
    "fake_en = Faker(\"en_US\")\n",
    "if SEED is not None:\n",
    "    fake_global.seed_instance(SEED)\n",
    "    fake_en.seed_instance(SEED + 1 if isinstance(SEED, int) else None)\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Make a filesystem-safe folder/file name.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[\\/\\\\:*?\\\"<>|]\", \"\", name)  # remove reserved characters\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    return name or \"unnamed\"\n",
    "\n",
    "def unique_items(generator, count):\n",
    "    \"\"\"Return up to 'count' unique items from a callable generator().\"\"\"\n",
    "    seen = set()\n",
    "    items = []\n",
    "    attempts = 0\n",
    "    while len(items) < count and attempts < count * 20:\n",
    "        attempts += 1\n",
    "        val = generator()\n",
    "        if val not in seen:\n",
    "            seen.add(val)\n",
    "            items.append(val)\n",
    "    return items\n",
    "\n",
    "def gen_words(n: int) -> str:\n",
    "    \"\"\"Generate n 'readable' lorem-like words with occasional punctuation.\"\"\"\n",
    "    words = fake_global.words(nb=n)\n",
    "    out = []\n",
    "    counter = 0\n",
    "    break_at = random.randint(12, 20)\n",
    "    for w in words:\n",
    "        out.append(w)\n",
    "        counter += 1\n",
    "        if counter >= break_at:\n",
    "            out[-1] = out[-1] + \".\"\n",
    "            counter = 0\n",
    "            break_at = random.randint(12, 20)\n",
    "    text = \" \".join(out)\n",
    "    text = \". \".join(s.strip().capitalize() for s in text.split(\".\") if s.strip())\n",
    "    if not text.endswith(\".\"):\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def generate_data():\n",
    "    ensure_dir(ROOT_DIR)\n",
    "\n",
    "    # Pre-calculate the directory structure\n",
    "    structure = []\n",
    "    total_files = 0\n",
    "\n",
    "    countries = unique_items(lambda: sanitize(fake_global.country()), TOP_LEVEL_COUNTRIES)\n",
    "\n",
    "    for country in countries:\n",
    "        country_dir = ROOT_DIR / country\n",
    "        num_cities = random.randint(1, MAX_CITIES_PER_COUNTRY)\n",
    "        cities = unique_items(lambda: sanitize(fake_global.city()), num_cities)\n",
    "\n",
    "        for city in cities:\n",
    "            city_dir = country_dir / city\n",
    "            num_first = random.randint(1, MAX_FIRSTNAMES_PER_CITY)\n",
    "            first_names = unique_items(lambda: sanitize(fake_en.first_name()), num_first)\n",
    "\n",
    "            for first_name in first_names:\n",
    "                first_dir = city_dir / first_name\n",
    "                num_last = random.randint(1, MAX_LASTNAMES_PER_FIRSTNAME)\n",
    "                last_names = unique_items(lambda: sanitize(fake_en.last_name()), num_last)\n",
    "\n",
    "                for last_name in last_names:\n",
    "                    leaf_dir = first_dir / last_name\n",
    "                    structure.append(leaf_dir)\n",
    "                    total_files += FILES_PER_LEAF\n",
    "\n",
    "    print(f\"Generating {total_files} files in {len(structure)} directories...\")\n",
    "\n",
    "    # Create all directories and files with a single progress bar\n",
    "    with tqdm(total=total_files, desc=\"Creating files\", unit=\"file\") as pbar:\n",
    "        for leaf_dir in structure:\n",
    "            ensure_dir(leaf_dir)\n",
    "\n",
    "            for i in range(1, FILES_PER_LEAF + 1):\n",
    "                fname = f\"file_{i:03d}.txt\"\n",
    "                fpath = leaf_dir / fname\n",
    "                word_count = random.randint(MIN_WORDS_PER_FILE, MAX_WORDS_PER_FILE)\n",
    "                content = gen_words(word_count)\n",
    "                fpath.write_text(content, encoding=\"utf-8\")\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"\\nâœ… Done. Created {total_files} files in structure under: {ROOT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342eb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the dir structure\n",
    "! tree -L 2 {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine some files\n",
    "! find {data_dir} | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87b132",
   "metadata": {},
   "source": [
    "## Create and push tokens to the database\n",
    "\n",
    "### Traverse the directory structure and create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dedf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "token_inputs = [fpath for fpath in glob.iglob(f\"{data_dir}/**/*.txt\", recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some of the paths\n",
    "from pprint import pprint\n",
    "fields = {\"input\": token_inputs}\n",
    "pprint(fields[\"input\"][:10])  # print first 10 paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97a8be",
   "metadata": {},
   "source": [
    "push the tokens to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picas.picas_config import PicasConfig\n",
    "from picas.crypto import decrypt_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9237c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PicasConfig(load=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection to the server\n",
    "from picas.clients import CouchDB\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e7762",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "db = CouchDB(\n",
    "    url=config.config['host_url'],\n",
    "    db=config.config['database'],\n",
    "    username=config.config['username'],\n",
    "    password=decrypt_password(config.config['encrypted_password']).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_tokens import create_tokens\n",
    "from time import perf_counter\n",
    "tokens = create_tokens(fields, offset=db.doc_count())\n",
    "pprint(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4b467",
   "metadata": {},
   "source": [
    "The push tokens script (save them to the database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f433bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "status = db.save_documents(tokens)\n",
    "elapsed = perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00922df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = len(tokens) / elapsed\n",
    "print(f\"Pushed {len(tokens)} in {elapsed:.3f}s ({rate:.1f} tokens/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of documents in the todo view\n",
    "print(f\"Number of documents in the todo view: {db.db.view('Monitor/todo').total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270b6f5",
   "metadata": {},
   "source": [
    "submit the job on spider to process all the tokens and do a word count on all the file\n",
    "! sbatch spider_example_word_count.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47934d",
   "metadata": {},
   "source": [
    "- navigate to the database web UI\n",
    "- check the done view\n",
    "- click on a token  and check the attachments (word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af954e",
   "metadata": {},
   "source": [
    "## Detailes of the example script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14049e2",
   "metadata": {},
   "source": [
    "Process the tokens by pulling tokens and using the PiCaS framework to run the tasks locally\n",
    "in each job on Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f1e33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from picas.actors import RunActor\n",
    "from picas.modifiers import BasicTokenModifier\n",
    "from picas.util import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9d809",
   "metadata": {},
   "source": [
    "The goal is to pass the parameters (tokens) to the .sh script and run it.\n",
    "PiCaS is responsible and will take care of fetching the tokens.\n",
    "The user's responsibility is implement the \"process_task\" method that PiCaS will call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f894d4",
   "metadata": {},
   "source": [
    "To customize the processing, the user needs to implement the process_task method\n",
    "https://github.com/sara-nl/picasclient/blob/master/examples/example_template.py\n",
    "see actual example in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bec65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleActor(RunActor):\n",
    "    \"\"\"\n",
    "    The ExampleActor is the custom implementation of a RunActor that the user needs for the processing.\n",
    "    Feel free to adjust to whatever you need, a template can be found at: example-template.py\n",
    "    \"\"\"\n",
    "    def __init__(self, db, modifier, view=\"todo\", **viewargs):\n",
    "        super(ExampleActor, self).__init__(db, view=view, **viewargs)\n",
    "        self.timer = Timer()\n",
    "        self.modifier = modifier\n",
    "        self.client = db\n",
    "\n",
    "    def process_task(self, token):\n",
    "        # Print token information\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Working on token: \" + token['_id'])\n",
    "        for key, value in token.doc.items():\n",
    "            print(key, value)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        data_fpath = token['input']\n",
    "\n",
    "        with open(data_fpath, 'r') as fobj:\n",
    "            text = fobj.read()\n",
    "            word_count = len(text.split())\n",
    "\n",
    "        self.subprocess = None\n",
    "\n",
    "        ## Get the job exit code and done in the token (since counting words is trivial)\n",
    "        ## assume it has succeeded\n",
    "        token['exit_code'] = 0\n",
    "        token = self.modifier.close(token)\n",
    "\n",
    "        ## Attach the word count\n",
    "        token.put_attachment('word_count', f'{word_count}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the token modifier\n",
    "modifier = BasicTokenModifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actor\n",
    "actor = ExampleActor(db, modifier, view='todo', design_doc='Monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the work!\n",
    "actor.run(max_token_time=1800, max_total_time=3600, max_tasks=100000, max_scrub=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d5218",
   "metadata": {},
   "source": [
    "- navigate to the database web UI\n",
    "- check the done view\n",
    "- click on a token  and check the attachments (word_count)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
